# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18AAXnWiIt2LcbOQitCndSawZmSWgJqJP
"""

pip install requests beautifulsoup4

import os
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import zipfile

df=pd.read_excel('/content/Input.xlsx')
df

pip install nltk

import nltk
nltk.download('vader_lexicon')
import nltk
nltk.download('cmudict')
import nltk
nltk.download('punkt')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import cmudict

# Extract stopwords from zip files
stopwords_folder = "/content/StopWords-20230831T163950Z-001.zip"  # Replace with the name of your stopwords zip file
stopwords = set()
with zipfile.ZipFile(stopwords_folder, "r") as stopwords_zip:
    for filename in stopwords_zip.namelist():
        with stopwords_zip.open(filename) as stopwords_file:
            try:
                content = stopwords_file.read().decode("utf-8")
            except UnicodeDecodeError:
                content = stopwords_file.read().decode("latin-1")  # Try a different encoding if utf-8 fails
            stopwords.update(word.strip() for word in content.splitlines())

# Extract positive words from positive-words.txt and negatve-wrds.txte
positive_words_file = "/content/positive-words.txt"
negative_words_file = "/content/negative-words.txt"
with open(positive_words_file, "r", encoding="latin-1") as pos_words_file:
    positive_words = set(pos_words_file.read().splitlines())

with open(negative_words_file, "r", encoding="latin-1") as neg_words_file:
    negative_words = set(neg_words_file.read().splitlines())
prondict = cmudict.dict()
personal_pronoun_pattern = r"\b(I|me|my|mine|myself|you|your|yours|yourself|he|him|his|himself|she|her|hers|herself|it|its|itself|we|us|our|ours|ourselves|they|them|their|theirs|themselves)\b"

# Function to count syllables in a word using CMU Pronouncing Dictionary
def count_syllables(word):
    if word.lower() in prondict:
        return max([len(list(y for y in x if y[-1].isdigit())) for x in prondict[word.lower()]])
    else:
        return 0
sia = SentimentIntensityAnalyzer()
# Analyze each URL
for index, row in df.iterrows():
    url = row["URL"]
    # Send a GET request to fetch the HTML content
    response = requests.get(url)
    html_content = response.content

    # Parse the HTML using BeautifulSoup
    soup = BeautifulSoup(html_content, "html.parser")
    article_container
    article_container = soup.find("div", class_="td-ss-main-content")
    #
    if not article_container:
         article_container = soup.find("div", class_="td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type")
    if article_container:
        # Extract the text content from the article container
        article_text = article_container.get_text()

        # Clean the article text by removing stopwords
        cleaned_text = " ".join([word for word in re.findall(r'\b\w+\b', article_text) if word.lower() not in stopwords])
        total_word_count = len(cleaned_text.split())
        total_word_length = sum(len(word) for word in cleaned_text.split())
        sentence_count = len(nltk.sent_tokenize(cleaned_text))
                # Count the occurrences of positive words
         # Perform sentiment analysis
       # sentiment_score = sia.polarity_scores(cleaned_text)
        positive_score= sum(1 for word in cleaned_text.split() if word.lower() in positive_words)
        negative_score =  sum(1 for word in cleaned_text.split() if word.lower() in negative_words)
        polarity_score = (positive_score -negative_score)/ ((positive_score + negative_score) + 0.000001)
        subjective_score=(positive_score +negative_score)/ ((total_word_count) + 0.000001)
        average_sentence_length =total_word_count/sentence_count
        complex_word_count = sum(1 for word in cleaned_text.split() if count_syllables(word) > 2)
        percent_complex=complex_word_count/total_word_count
        fog_index=0.4*(percent_complex+average_sentence_length)
        total_syllables=sum(count_syllables(word) for word in cleaned_text.split() )
        personal_pronoun_count = len(re.findall(personal_pronoun_pattern, cleaned_text, re.IGNORECASE))
        df.at[index, "Positive Score"] = positive_score
        df.at[index, "Negative Score"] = negative_score
        df.at[index, "Polarity Score"] = polarity_score
        df.at[index,'Subjectivity Score']=subjective_score
        df.at[index,'Average Sentence Length']=average_sentence_length
        df.at[index,'Percentage of Complex Words']=percent_complex
        df.at[index,'Fog index']=fog_index
        df.at[index, 'Average Number of Words per sentence']=total_word_count/sentence_count
        df.at[index,'Complex Word Count']=complex_word_count
        df.at[index,'Word Count']=total_word_count
        df.at[index,'Syllable per word']=total_word_count/total_syllables
        df.at[index,'Personal Pronoun']=personal_pronoun_count
        df.at[index,'Average Word Length']=total_word_length/total_word_count if total_word_count>0 else 0
    else:
        df.at[index, "Positive Score"] = 0
        df.at[index, "Negative Score"] = 0
        df.at[index, "Polarity Score"] = 0
        df.at[index,'Subjectivity Score']=0
        df.at[index,'Average Sentence Length']=0
        df.at[index,'Percentage of Complex Words']=0
        df.at[index,'Fog index']=0
        df.at[index, 'Average Number of Words per sentence']=0
        df.at[index,'Complex Word Count']=0
        df.at[index,'Word Count']=0
        df.at[index,'Syllable per word']=0
        df.at[index,'Personal Pronoun']=0
        df.at[index,'Average Word Length']=0

df

df.to_excel("Output.xlsx", index=False)